{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb32c28",
   "metadata": {},
   "source": [
    "IT5437 – Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup: installs & imports ===\n",
    "!pip -q install opencv-python-headless==4.10.0.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "\n",
    "def load_image(path):\n",
    "    \"\"\"Load an image from path (returns BGR).\"\"\"\n",
    "    img = cv.imread(str(path), cv.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "    return img\n",
    "\n",
    "def show(img, title=None, cmap=None):\n",
    "    plt.figure()\n",
    "    if img.ndim == 2:\n",
    "        plt.imshow(img, cmap=cmap or 'gray')\n",
    "    else:\n",
    "        plt.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_hist(img, title=\"Histogram\", color=True):\n",
    "    plt.figure()\n",
    "    if img.ndim == 2:\n",
    "        plt.hist(img.ravel(), bins=256, range=(0,255))\n",
    "    else:\n",
    "        if color:\n",
    "            for i, col in enumerate((\"b\",\"g\",\"r\")):\n",
    "                plt.hist(img[:,:,i].ravel(), bins=256, range=(0,255), alpha=0.5, label=col)\n",
    "            plt.legend()\n",
    "        else:\n",
    "            gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "            plt.hist(gray.ravel(), bins=256, range=(0,255))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def to_uint8(x):\n",
    "    return np.clip(np.round(x), 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c709dbb4",
   "metadata": {},
   "source": [
    "Q1. Intensity transformation (apply Fig. 1a to Fig. 1b) Define the curve by control points and apply via LUT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297de068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 – Intensity Transformation\n",
    "img = load_image(r\"emma.jpg\")\n",
    "show(img, \"Q1: Input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5025cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define control points (adjust to match Fig. 1a)\n",
    "control_x = np.array([0, 50, 100, 150, 200, 255], dtype=np.float32)\n",
    "control_y = np.array([0, 30, 120, 180, 220, 255], dtype=np.float32)\n",
    "\n",
    "x = np.arange(256, dtype=np.float32)\n",
    "lut = np.interp(x, control_x, control_y)\n",
    "lut_u8 = to_uint8(lut)\n",
    "\n",
    "out = cv.LUT(img, lut_u8)\n",
    "show(out, \"Q1: Transformed\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, lut, lw=2)\n",
    "plt.title(\"Q1: Intensity Transformation Curve\")\n",
    "plt.xlabel(\"Input intensity\")\n",
    "plt.ylabel(\"Output intensity\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309eb6d7",
   "metadata": {},
   "source": [
    "Q2. Accentuate white & gray matter (Fig. 2) Use separate LUTs to emphasize high (white matter) and mid (gray matter) intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 – Brain proton density image (Fig. 2)\n",
    "brain = load_image(\"brain_proton_density_slice.png\")  \n",
    "# Convert to grayscale if image is RGB\n",
    "if brain.ndim == 3:\n",
    "    brain_gray = cv.cvtColor(brain, cv.COLOR_BGR2GRAY)\n",
    "else:\n",
    "    brain_gray = brain.copy()\n",
    "\n",
    "# Show result\n",
    "show(brain_gray, \"Q2: Brain (gray)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18289be",
   "metadata": {},
   "outputs": [],
   "source": [
    "wm_low, wm_high = 140, 255\n",
    "x = np.arange(256, dtype=np.float32)\n",
    "wm_lut = np.piecewise(x,\n",
    "    [x<wm_low, (x>=wm_low)&(x<=wm_high), x>wm_high],\n",
    "    [0, lambda t: 255*(t-wm_low)/(wm_high-wm_low+1e-6), 255])\n",
    "wm = cv.LUT(brain_gray, to_uint8(wm_lut))\n",
    "\n",
    "gm_c, gm_w = 120, 60\n",
    "gm_low, gm_high = gm_c-gm_w, gm_c+gm_w\n",
    "gm_lut = np.piecewise(x,\n",
    "    [x<gm_low, (x>=gm_low)&(x<=gm_high), x>gm_high],\n",
    "    [0, lambda t: 255*(t-gm_low)/(gm_high-gm_low+1e-6), 0])\n",
    "gm = cv.LUT(brain_gray, to_uint8(gm_lut))\n",
    "\n",
    "show(wm, \"Q2: White matter accentuated\")\n",
    "show(gm, \"Q2: Gray matter accentuated\")\n",
    "plt.figure(); plt.plot(x, wm_lut); plt.title(\"Q2: WM LUT\"); plt.grid(True); plt.show()\n",
    "plt.figure(); plt.plot(x, gm_lut); plt.title(\"Q2: GM LUT\"); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d007b88",
   "metadata": {},
   "source": [
    "Q3. Gamma correction on L* in Lab (Fig. 3) Apply gamma to L, then show histograms before/after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3994452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 – Gamma correction on L* in Lab space\n",
    "img3 = load_image(\"highlights_and_shadows.jpg\")\n",
    "\n",
    "# Convert to Lab\n",
    "lab = cv.cvtColor(img3, cv.COLOR_BGR2LAB)\n",
    "L, A, B = cv.split(lab)\n",
    "\n",
    "# Apply gamma correction to L\n",
    "gamma = 0.8  # adjust & report\n",
    "Ln = L.astype(np.float32) / 255.0\n",
    "Lcorr = to_uint8((Ln ** gamma) * 255.0)\n",
    "\n",
    "# Recombine channels\n",
    "out3 = cv.cvtColor(cv.merge([Lcorr, A, B]), cv.COLOR_LAB2BGR)\n",
    "\n",
    "# Show results\n",
    "show(img3, \"Q3: Original\")\n",
    "show(out3, f\"Q3: Gamma-corrected (gamma={gamma})\")\n",
    "\n",
    "# Show histograms\n",
    "plot_hist(cv.cvtColor(img3, cv.COLOR_BGR2GRAY), \"Q3: Original hist\", color=False)\n",
    "plot_hist(cv.cvtColor(out3, cv.COLOR_BGR2GRAY), \"Q3: Corrected hist\", color=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d7add6",
   "metadata": {},
   "source": [
    "Q4. Vibrance via saturation transform (Fig. 4) f(x) = min(x + a * 128 * exp(-(x-128)^2/(2*sigma^2)), 255), sigma=70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0606af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 – Vibrance enhancement (apply to saturation channel)\n",
    "img4 = load_image(\"spider.png\") \n",
    "\n",
    "# Convert to HSV\n",
    "hsv = cv.cvtColor(img4, cv.COLOR_BGR2HSV)\n",
    "H, S, V = cv.split(hsv)\n",
    "\n",
    "# Define transformation parameters\n",
    "sigma = 70.0\n",
    "a = 0.6  # adjust & report\n",
    "x = np.arange(256, dtype=np.float32)\n",
    "\n",
    "# Compute vibrance LUT\n",
    "bump = a * 128.0 * np.exp(-((x - 128.0) ** 2) / (2 * sigma ** 2))\n",
    "lut = np.minimum(x + bump, 255.0)\n",
    "\n",
    "# Apply LUT to saturation channel\n",
    "S2 = cv.LUT(S, to_uint8(lut))\n",
    "\n",
    "# Recombine HSV → BGR\n",
    "out4 = cv.cvtColor(cv.merge([H, S2, V]), cv.COLOR_HSV2BGR)\n",
    "\n",
    "# Show results\n",
    "show(img4, \"Q4: Original\")\n",
    "show(out4, f\"Q4: Vibrance enhanced (a={a}, sigma={sigma})\")\n",
    "\n",
    "# Plot the intensity transformation\n",
    "plt.figure()\n",
    "plt.plot(x, lut, lw=2)\n",
    "plt.title(\"Q4: Saturation Transform\")\n",
    "plt.xlabel(\"Input S\")\n",
    "plt.ylabel(\"Output S\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6502641",
   "metadata": {},
   "source": [
    "Q5. Foreground-only histogram equalization (Fig. 5) Threshold (V via Otsu), equalize only foreground using cumsum, recombine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 – Foreground-only histogram equalization\n",
    "img5 = load_image(\"jeniffer.jpg\")  \n",
    "\n",
    "# Convert to HSV\n",
    "hsv5 = cv.cvtColor(img5, cv.COLOR_BGR2HSV)\n",
    "H, S, V = cv.split(hsv5)\n",
    "\n",
    "# Show planes\n",
    "show(H, \"Q5: H plane (grayscale)\")\n",
    "show(S, \"Q5: S plane (grayscale)\")\n",
    "show(V, \"Q5: V plane (grayscale)\")\n",
    "\n",
    "# Threshold V using Otsu to get mask\n",
    "_, mask = cv.threshold(V, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "show(mask, \"Q5: Foreground Mask\")\n",
    "\n",
    "# Foreground-only histogram equalization\n",
    "fg = cv.bitwise_and(V, V, mask=mask)\n",
    "hist = cv.calcHist([fg], [0], mask, [256], [0,256]).ravel()\n",
    "cdf = np.cumsum(hist)\n",
    "cdf_norm = (cdf - cdf.min()) / (cdf.max() - cdf.min() + 1e-6) * 255.0\n",
    "lut = to_uint8(cdf_norm)\n",
    "\n",
    "V_eq = V.copy()\n",
    "V_eq[mask > 0] = lut[V[mask > 0]]\n",
    "\n",
    "# Recombine HSV → BGR\n",
    "out5 = cv.cvtColor(cv.merge([H, S, V_eq]), cv.COLOR_HSV2BGR)\n",
    "\n",
    "# Show results\n",
    "show(img5, \"Q5: Original\")\n",
    "show(out5, \"Q5: Foreground Equalized Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686926d7",
   "metadata": {},
   "source": [
    "Q6. Sobel filtering (filter2D, manual, separable) on Fig. 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 – Sobel filtering (Input image)\n",
    "img6 = load_image(\"einstein.png\") \n",
    "# Convert to grayscale\n",
    "gray6 = cv.cvtColor(img6, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Show result\n",
    "show(gray6, \"Q6: Input (grayscale)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef977f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kx = np.array([[1,0,-1],[2,0,-2],[1,0,-1]], np.float32)\n",
    "Ky = Kx.T\n",
    "Gx = cv.filter2D(gray6, cv.CV_32F, Kx)\n",
    "Gy = cv.filter2D(gray6, cv.CV_32F, Ky)\n",
    "mag1 = cv.magnitude(Gx, Gy)\n",
    "show(to_uint8(mag1), \"Q6: Magnitude (filter2D)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2(img, kernel):\n",
    "    kh, kw = kernel.shape\n",
    "    pad_h, pad_w = kh//2, kw//2\n",
    "    padded = cv.copyMakeBorder(img, pad_h, pad_h, pad_w, pad_w, cv.BORDER_REFLECT)\n",
    "    out = np.zeros_like(img, dtype=np.float32)\n",
    "    k = np.flipud(np.fliplr(kernel))\n",
    "    for y in range(out.shape[0]):\n",
    "        for x in range(out.shape[1]):\n",
    "            out[y,x] = np.sum(padded[y:y+kh, x:x+kw] * k)\n",
    "    return out\n",
    "\n",
    "Gx2 = conv2(gray6.astype(np.float32), Kx)\n",
    "Gy2 = conv2(gray6.astype(np.float32), Ky)\n",
    "mag2 = cv.magnitude(Gx2, Gy2)\n",
    "show(to_uint8(mag2), \"Q6: Magnitude (manual)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([[1],[2],[1]], np.float32)\n",
    "h = np.array([[1,0,-1]], np.float32)\n",
    "Gx3 = cv.filter2D(cv.filter2D(gray6, cv.CV_32F, v), cv.CV_32F, h)\n",
    "Gy3 = cv.filter2D(cv.filter2D(gray6, cv.CV_32F, h.T), cv.CV_32F, v.T)\n",
    "mag3 = cv.magnitude(Gx3, Gy3)\n",
    "show(to_uint8(mag3), \"Q6: Magnitude (separable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b23c47",
   "metadata": {},
   "source": [
    "Q7. Image zooming (nearest & bilinear) + NSSD vs original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 – Image zooming (nearest & bilinear) + NSSD comparison\n",
    "\n",
    "# Load images \n",
    "small = load_image(r\"a1q8images/im01small.png\")  \n",
    "orig  = load_image(r\"a1q8images/im01.png\")    \n",
    "\n",
    "s = 4.0  # scale factor\n",
    "\n",
    "# Zoom function\n",
    "def zoom(img, s, method='nearest'):\n",
    "    h, w = img.shape[:2]\n",
    "    new_size = (int(w*s), int(h*s))\n",
    "    interp = cv.INTER_NEAREST if method == 'nearest' else cv.INTER_LINEAR\n",
    "    return cv.resize(img, new_size, interpolation=interp)\n",
    "\n",
    "# Upscale with both methods\n",
    "up_nearest = zoom(small, s, 'nearest')\n",
    "up_bilinear = zoom(small, s, 'bilinear')\n",
    "\n",
    "# Crop to common size for comparison\n",
    "H = min(orig.shape[0], up_bilinear.shape[0])\n",
    "W = min(orig.shape[1], up_bilinear.shape[1])\n",
    "A  = orig[:H, :W]\n",
    "Bn = up_nearest[:H, :W]\n",
    "Bl = up_bilinear[:H, :W]\n",
    "\n",
    "# Normalized Sum of Squared Differences (NSSD)\n",
    "def nssd(a, b):\n",
    "    a32 = a.astype(np.float32)\n",
    "    b32 = b.astype(np.float32)\n",
    "    return np.mean((a32 - b32) ** 2) / (np.mean(a32 ** 2) + 1e-8)\n",
    "\n",
    "print(\"NSSD (nearest): \", nssd(A, Bn))\n",
    "print(\"NSSD (bilinear):\", nssd(A, Bl))\n",
    "\n",
    "# Show results\n",
    "show(up_nearest, \"Q7: Upscaled (nearest)\")\n",
    "show(up_bilinear, \"Q7: Upscaled (bilinear)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43978dda",
   "metadata": {},
   "source": [
    "Q8. GrabCut segmentation & background blur (Fig. 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b57de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 – GrabCut segmentation + background blur\n",
    "\n",
    "# Load image\n",
    "img8 = load_image(\"daisy.jpg\")\n",
    "\n",
    "# Define initial rectangle (slightly inside the full image)\n",
    "h, w = img8.shape[:2]\n",
    "rect = (int(0.05 * w), int(0.05 * h), int(0.9 * w), int(0.9 * h))\n",
    "\n",
    "# Initialize mask and models\n",
    "mask = np.zeros((h, w), np.uint8)\n",
    "bgdModel = np.zeros((1, 65), np.float64)\n",
    "fgdModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "# Apply GrabCut\n",
    "cv.grabCut(img8, mask, rect, bgdModel, fgdModel, 5, cv.GC_INIT_WITH_RECT)\n",
    "mask2 = np.where((mask == cv.GC_FGD) | (mask == cv.GC_PR_FGD), 255, 0).astype('uint8')\n",
    "\n",
    "# Extract foreground and background\n",
    "fg = cv.bitwise_and(img8, img8, mask=mask2)\n",
    "bg_mask = cv.bitwise_not(mask2)\n",
    "bg = cv.bitwise_and(img8, img8, mask=bg_mask)\n",
    "\n",
    "# Blur background\n",
    "bg_blur = cv.GaussianBlur(bg, (0, 0), 9)\n",
    "\n",
    "# Combine foreground with blurred background\n",
    "enhanced = cv.add(fg, bg_blur)\n",
    "\n",
    "# Show results\n",
    "show(mask2, \"Q8: Segmentation Mask\")\n",
    "show(fg, \"Q8: Foreground\")\n",
    "show(bg, \"Q8: Background\")\n",
    "show(np.hstack([img8, enhanced]), \"Q8: Original | Enhanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d75d01",
   "metadata": {},
   "source": [
    "Q8(c) Answer: The dark halo just beyond the flower edge is due to imperfect GrabCut boundaries and hard compositing: ambiguous edge pixels are assigned to background, then blurred and recombined, creating a darker rim where background adjoins the foreground."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f66f8",
   "metadata": {},
   "source": [
    "Q9. Rice grain counting (Fig. 8a, 8b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img9a= load_image(\"daisy.jpg\")\n",
    "img9b= load_image(\"daisy.jpg\")\n",
    "\n",
    "ga = cv.cvtColor(img9a, cv.COLOR_BGR2GRAY)\n",
    "gb = cv.cvtColor(img9b, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "ga_dn = cv.GaussianBlur(ga, (5,5), 1.0)\n",
    "gb_dn = cv.medianBlur(gb, 3)\n",
    "\n",
    "_, ta = cv.threshold(ga_dn, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "_, tb = cv.threshold(gb_dn, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "\n",
    "kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE, (3,3))\n",
    "ta_clean = cv.morphologyEx(ta, cv.MORPH_OPEN, kernel, iterations=2)\n",
    "ta_clean = cv.morphologyEx(ta_clean, cv.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "tb_clean = cv.morphologyEx(tb, cv.MORPH_OPEN, kernel, iterations=2)\n",
    "tb_clean = cv.morphologyEx(tb_clean, cv.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "cc_a, labels_a, stats_a, _ = cv.connectedComponentsWithStats(ta_clean, connectivity=8)\n",
    "cc_b, labels_b, stats_b, _ = cv.connectedComponentsWithStats(tb_clean, connectivity=8)\n",
    "\n",
    "count_a = stats_a.shape[0]-1\n",
    "count_b = stats_b.shape[0]-1\n",
    "\n",
    "show(ga_dn, \"Q9a: Denoised (Gaussian)\")\n",
    "show(ta_clean, f\"Q9a: Segmented (count={count_a})\")\n",
    "show(gb_dn, \"Q9b: Denoised (Median)\")\n",
    "show(tb_clean, f\"Q9b: Segmented (count={count_b})\")\n",
    "\n",
    "print(\"Q9a grains (approx):\", count_a)\n",
    "print(\"Q9b grains (approx):\", count_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd3d66",
   "metadata": {},
   "source": [
    "Q10. Sapphire areas (Fig. 9) with connected components and geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d2f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img10= load_image(\"jeniffer.jpg\")\n",
    "gray10 = cv.cvtColor(img10, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "g = cv.GaussianBlur(gray10, (5,5), 0)\n",
    "_, th = cv.threshold(g, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "\n",
    "# Ensure sapphires are white in mask\n",
    "if np.mean(g[th>0]) < np.mean(g[th==0]):\n",
    "    mask = th\n",
    "else:\n",
    "    mask = cv.bitwise_not(th)\n",
    "\n",
    "mask_filled = cv.morphologyEx(mask, cv.MORPH_CLOSE, cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5)), iterations=3)\n",
    "mask_filled = cv.morphologyEx(mask_filled, cv.MORPH_OPEN, cv.getStructuringElement(cv.MORPH_ELLIPSE,(3,3)), iterations=1)\n",
    "\n",
    "num, labels, stats, centroids = cv.connectedComponentsWithStats(mask_filled, connectivity=8)\n",
    "areas_px = stats[1:, cv.CC_STAT_AREA]\n",
    "\n",
    "show(mask, \"Q10: Initial mask\")\n",
    "show(mask_filled, \"Q10: After hole filling\")\n",
    "print(\"Pixel areas:\", areas_px.tolist())\n",
    "\n",
    "# Real-world conversion parameters\n",
    "f_mm = 8.0\n",
    "Z_mm = 480.0\n",
    "pixel_pitch_mm = 0.005  # 5 µm; set to your camera\n",
    "\n",
    "s_mm = (Z_mm / f_mm) * pixel_pitch_mm\n",
    "area_per_pixel_mm2 = s_mm**2\n",
    "areas_mm2 = areas_px * area_per_pixel_mm2\n",
    "\n",
    "for i, (apx, amm2) in enumerate(zip(areas_px, areas_mm2), start=1):\n",
    "    print(f\"Object {i}: area_pixels={int(apx)}, area_mm2≈{amm2:.2f} (pixel_pitch={pixel_pitch_mm*1000:.1f} µm)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
